{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4782ed39-f2b3-4038-adc1-c541641dc28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DONE ===\n",
      "UNIQUE: 4684\n",
      "DUPLICATES: 166\n",
      "FOREIGN LANGUAGE DETECTED: 33\n"
     ]
    }
   ],
   "source": [
    "# fast_near_duplicate_simhash_langdetect_clean.py\n",
    "\n",
    "from simhash import Simhash, SimhashIndex\n",
    "from collections import Counter\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "INPUT_FILE = \"D:/Document/AI Talent Factory/TIM 1/Data-All/Gabungan-All.txt\"\n",
    "UNIQUE_FILE = \"D:/Document/AI Talent Factory/TIM 1/Data-All/unique_Gabungan-All.txt\"\n",
    "DUP_FILE = \"D:/Document/AI Talent Factory/TIM 1/Data-All/duplicate-Gabungan-All.txt\"\n",
    "\n",
    "SIMHASH_K = 3\n",
    "MAX_TOKEN_WEIGHT = 10\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# CLEAN-UP: hapus sumber artikel seperti \"Liputan6.com, Jakarta\"\n",
    "# --------------------------------------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    remove_list = [\n",
    "        \"Liputan6.com, Jakarta\",\n",
    "        \"Liputan6.com , Jakarta\",\n",
    "        \"LIPUTAN6.COM, JAKARTA\",\n",
    "        \"liputan6.com, jakarta\",\n",
    "        \"liputan6.com , jakarta\",\n",
    "        \"REPUBLIKA.CO.ID\",\n",
    "        \"JAKARTA, KOMPAS.com-\",\n",
    "        \"Bisnis.com, JAKARTA\"\n",
    "    ]\n",
    "    for r in remove_list:\n",
    "        text = text.replace(r, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def detect_language_safe(text: str) -> str:\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def build_simhash(text: str) -> Simhash:\n",
    "    tokens = text.lower().split()\n",
    "    counts = Counter(tokens)\n",
    "    features = []\n",
    "\n",
    "    for tok, cnt in counts.items():\n",
    "        w = cnt if cnt <= MAX_TOKEN_WEIGHT else MAX_TOKEN_WEIGHT\n",
    "        features.append((tok, w))\n",
    "\n",
    "    return Simhash(features)\n",
    "\n",
    "\n",
    "def main():\n",
    "    index = SimhashIndex([], k=SIMHASH_K)\n",
    "\n",
    "    unique = 0\n",
    "    dups = 0\n",
    "    foreign = 0\n",
    "\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(UNIQUE_FILE, \"w\", encoding=\"utf-8\") as fout_u, \\\n",
    "         open(DUP_FILE, \"w\", encoding=\"utf-8\") as fout_d:\n",
    "\n",
    "        for lineno, line in enumerate(fin, start=1):\n",
    "            text = line.strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # --------------------------------------------------------\n",
    "            # STEP 1 — CLEAN-UP sumber artikel\n",
    "            # --------------------------------------------------------\n",
    "            text = clean_text(text)\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # --------------------------------------------------------\n",
    "            # STEP 2 — DETEKSI BAHASA\n",
    "            # --------------------------------------------------------\n",
    "            lang = detect_language_safe(text)\n",
    "\n",
    "            if lang != \"id\":\n",
    "                fout_d.write(f\"[LANG={lang}] \" + text + \"\\n\\n\")\n",
    "                foreign += 1\n",
    "                dups += 1\n",
    "                continue\n",
    "\n",
    "            # --------------------------------------------------------\n",
    "            # STEP 3 — SIMHASH\n",
    "            # --------------------------------------------------------\n",
    "            sh = build_simhash(text)\n",
    "            near = index.get_near_dups(sh)\n",
    "\n",
    "            if near:\n",
    "                fout_d.write(text + \"\\n\\n\")\n",
    "                dups += 1\n",
    "            else:\n",
    "                key = f\"l{lineno}\"\n",
    "                index.add(key, sh)\n",
    "\n",
    "                fout_u.write(text + \"\\n\\n\")\n",
    "                unique += 1\n",
    "\n",
    "            if lineno % 100000 == 0:\n",
    "                print(f\"Processed {lineno} lines... (unique={unique}, dups={dups}, foreign={foreign})\")\n",
    "\n",
    "    print(\"\\n=== DONE ===\")\n",
    "    print(\"UNIQUE:\", unique)\n",
    "    print(\"DUPLICATES:\", dups)\n",
    "    print(\"FOREIGN LANGUAGE DETECTED:\", foreign)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aitf)",
   "language": "python",
   "name": "aitf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
