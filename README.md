# ğŸ§  Web Scraper for LLM Continued Pre-Training (CPT) Datasets

## ğŸ¯ Definition & Primary Goal
This system is not merely a standard web page downloader. It is designed specifically as a **Data Engineering Pipeline** to collect, clean, and standardize unstructured text data from the internet.

The primary goal is to compile a high-quality **Corpus** to be used for re-training (continued pre-training) Large Language Models (such as Llama 3, Mistral, or Qwen) so that they master **specific knowledge domains** or **specific languages** they previously lacked proficiency in.

## âš™ï¸ Why is this Scraper Critical for CPT?

In the LLM lifecycle, *Continued Pre-Training* differs from *Fine-Tuning*.
* **Fine-Tuning** teaches the model "how to answer" (instruction following).
* **Continued Pre-Training** teaches the model "new knowledge/facts" (knowledge injection).

This scraper fills those knowledge gaps by ensuring:

### 1. Domain Adaptation
Base Models are often trained on general English data. This scraper targets specific keywords (defined in `keywords.py`) to capture *niche* data, for example:
* Indonesian legal regulations.
* Technical industry terms (e.g., Telecommunications, Oil & Gas).
* Breaking news and local cultural contexts.

### 2. Noise Reduction
LLMs are highly sensitive to "garbage" data. Training a model with HTML tags, ads, or website navigation menus causes model damage (performance degradation).
The `paralel_elemen_html.py` script performs **Aggressive Data Sanitation**:
* Removes non-content elements (Ads, "Read Also", Sidebars).
* Cleans *boilerplate* (Headers/Footers).
* Normalizes text formatting to be ready for the Tokenizer.

### 3. High-Volume Data Collection
CPT requires millions to billions of tokens. The **Asynchronous (Asyncio)** and **Dockerized** architecture used here allows for massive, parallel data retrieval without excessively burdening local resources.

---

## ğŸ”„ Data Transformation Pipeline

Here is how raw data is transformed into knowledge for AI:

1.  **Targeting (`keywords.py` & `paralel_link_page.py`)**
    * **Function:** Determines what topics the model needs to learn.
    * **Output:** A list of search result pages relevant to the target domain.

2.  **Filtering (`folder_paralel_link_web.py`)**
    * **Function:** Filters out junk URLs and retains only valid article URLs.
    * **LLM Context:** Ensures the model learns only from authoritative sources, not from spam pages.

3.  **Extraction & Cleaning (`paralel_elemen_html.py`)**
    * **Function:** Extracts the "meat" (main content) of the article.
    * **Importance for LLM:** This script removes HTML tags and merges paragraphs. The final result is clean **Plain Text** (`.txt`).

4.  **Training Ready**
    * The `cpt_ready.txt` output file can be directly fed into the **Tokenization** -> **Embedding** -> **Model Training** process.

---

## ğŸ’¡ Simple Analogy

Imagine the LLM is an **English Literature Student**.
You want this student to work as a **Law Expert in Indonesia**.

* **This Web Scraper** is the research team going to libraries and courts.
* **Filtering & Cleaning** is the process of tearing out ad pages, throwing away damaged book covers, and keeping only the neat text of legal articles.
* **Continued Pre-Training** is the process of making that student read (study) all the organized legal texts day and night.

**The Result:** The English Literature student now understands Indonesian legal terms.

---

## ğŸ“Š Output Data Quality Specifications

The data generated by this scraper meets *LLM Training Readiness* standards:
* **Format:** UTF-8 Plain Text.
* **Structure:** One article per line (or separated by double newlines), facilitating *batch processing*.
* **Cleanliness:** Free from JavaScript code, CSS, and HTML tags that could confuse the model during *Next Token Prediction*.

Here is the complete setup tutorial in English, tailored for collecting data for **LLM Continued Pre-Training**. This guide incorporates all the fixes (version pinning, headless mode, and task sequencing) discussed previously.

-----

# ğŸš€ Setup Guide: Web Scraper for LLM Continued Pre-Training

This tutorial will guide you through setting up a Dockerized environment to scrape, filter, and clean web data. The output is a clean text corpus optimized for injecting new domain knowledge into Large Language Models (LLMs).

## ğŸ“‹ Prerequisites

Before starting, ensure you have the following installed:

  * **Docker Desktop** (Windows/Mac) or **Docker Engine** (Linux).
      * *Status must be "Running".*
  * **Git** (Optional, for cloning repositories).

-----

## ğŸ› ï¸ Step-by-Step Configuration

### 1\. Prepare Project Structure

Create a new folder (e.g., `CPT_Scraper`) and ensure your file structure looks exactly like this:

```text
CPT_Scraper/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Taskfile.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ keywords.py
â”œâ”€â”€ paralel_link_page.py
â”œâ”€â”€ folder_paralel_link_web.py
â””â”€â”€ paralel_elemen_html.py
```

### 2\. Create `Dockerfile`

This defines the environment. We use a specific version of the Playwright Python image to ensure stability.

**File:** `Dockerfile`

```dockerfile
# Use the official Playwright Python image (Pinned to v1.48.0 for stability)
FROM mcr.microsoft.com/playwright/python:v1.48.0-jammy

# Set working directory inside the container
WORKDIR /app

# Update apt repositories and install Task (Go-Task)
RUN apt-get update && \
    sh -c "$(curl --location https://taskfile.dev/install.sh)" -- -d -b /usr/local/bin

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application code
COPY . .

# Create output directories to prevent permission issues
RUN mkdir -p data/page data/url data/results

# Default command to run the scraper sequence
CMD ["/usr/local/bin/task", "run"]
```

### 3\. Create `requirements.txt`

We must pin the Playwright library version to match the Docker image (`1.48.0`) to avoid driver incompatibility errors.

**File:** `requirements.txt`

```text
playwright==1.48.0
beautifulsoup4
lxml
html5lib
tqdm
nest_asyncio
deep-translator
simhash
langdetect
```

### 4\. Create `Taskfile.yml`

This orchestrates the execution order. We use `cmds` (sequential) instead of `deps` (parallel) to ensure data dependencies are met.

**File:** `Taskfile.yml`

```yaml
version: '3'

tasks:
  paralel_link_page:
    desc: "Step 1: Search Bing and scrape pagination links"
    cmds:
      - python paralel_link_page.py

  folder_paralel_link_web:
    desc: "Step 2: Extract actual article URLs from search pages"
    cmds:
      - python folder_paralel_link_web.py

  paralel_elemen_html:
    desc: "Step 3: Download content and clean text for LLM"
    cmds:
      - python paralel_elemen_html.py

  run:
    desc: "Run the full CPT scraping pipeline sequentially"
    cmds:
      - task: paralel_link_page
      - task: folder_paralel_link_web
      - task: paralel_elemen_html
```

### 5\. Create `docker-compose.yml`

This configures the volume mapping so the scraped data appears on your local machine.

**File:** `docker-compose.yml`

```yaml
services:
  scraper:
    build: .
    container_name: scraping_bot
    volumes:
      # Maps the internal /app/data folder to your local ./data folder
      - ./data:/app/data
    environment:
      # Define explicit paths for the scripts to use
      - OUTPUT_DIR=data/page
      - INPUT_DIR_PAGE=data/page
      - OUTPUT_CSV_LINK=data/url/link.csv
      - INPUT_CSV_LINK=data/url/link.csv
      - OUTPUT_TXT_RESULT=data/results/cpt_ready.txt
    # Increase shared memory for browser stability
    shm_size: '2gb'
```

### 6\. âš ï¸ Crucial Code Adjustment: Headless Mode

Because Docker is a server environment (no monitor), you must ensure all your Python scripts use `headless=True`.

Check your scripts (`paralel_link_page.py`, `folder_paralel_link_web.py`, `paralel_elemen_html.py`) and update the browser launch code:

```python
# INCORRECT (Will crash in Docker):
# browser = await pw.chromium.launch(headless=False)

# CORRECT:
browser = await pw.chromium.launch(headless=True)
```

-----

## ğŸƒâ€â™‚ï¸ How to Run

1.  Open your terminal (Command Prompt, PowerShell, or Terminal).

2.  Navigate to the project directory.

3.  Run the following command to build the image and start the scraper:

    ```bash
    docker-compose up --build
    ```

### What Happens Next?

1.  **Building**: Docker downloads the Python+Playwright image and installs dependencies.
2.  **Running**: The `task run` command executes the scripts in order:
      * **Step 1**: Searches keywords on Bing and saves page links.
      * **Step 2**: Extracts clean article URLs.
      * **Step 3**: Downloads the article text, removes HTML/Ads, and saves it.
3.  **Completion**: Once finished, the container will exit.

## ğŸ“‚ accessing the Data

You do not need to enter the container. The data will appear automatically in your project folder:

  * **Raw Search Pages**: `data/page/`
  * **Clean URL List**: `data/url/link.csv`
  * **Final Training Data**: `data/results/cpt_ready.txt`

You can now use `cpt_ready.txt` to continue pre-training your LLM\! ğŸ¤–